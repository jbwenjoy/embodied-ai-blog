---
layout: article
title: IRL
permalink: /irl/
---


é€†å¼ºåŒ–å­¦ä¹ ï¼ˆ**Inverse Reinforcement Learning**ï¼Œç®€ç§° **IRL**ï¼‰æ˜¯ä¸€ç§åœ¨**å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**æ¡†æ¶ä¸­çš„ç ”ç©¶æ–¹æ³•ï¼Œå®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š



â€‹	**å·²çŸ¥ä¸€ä¸ªæ™ºèƒ½ä½“çš„è¡Œä¸ºè½¨è¿¹ï¼Œæ¨æµ‹å‡ºå®ƒèƒŒåéšå«çš„å¥–åŠ±å‡½æ•°ï¼ˆæˆ–ç›®æ ‡ï¼‰ã€‚**



------



**ğŸš— æ‰“ä¸ªç®€å•çš„æ¯”æ–¹ï¼š**



å‡è®¾ä½ æ˜¯ä¸ªé©¾æ ¡è€å¸ˆï¼Œçœ‹åˆ°äº†ä¸€ä¸ªè€å¸æœºåœ¨ä¸åŒæƒ…å†µä¸‹å¼€è½¦ï¼ˆæ¯”å¦‚åŠ é€Ÿã€åˆ¹è½¦ã€é¿è®©ç­‰ï¼‰ã€‚ä½ ä¸çŸ¥é“ä»–è„‘å­é‡Œå…·ä½“çš„**â€œå¼€è½¦å‡†åˆ™â€**ï¼ˆä¹Ÿå°±æ˜¯å¥–åŠ±å‡½æ•°ï¼‰ï¼Œä½†ä½ èƒ½çœ‹åˆ°ä»–çš„è¡Œä¸ºã€‚é‚£ä¹ˆä½ å°±å¯ä»¥é€šè¿‡ é€†å¼ºåŒ–å­¦ä¹ ï¼Œå°è¯•**åæ¨**ä»–åšè¿™äº›æ“ä½œæ˜¯ä¸ºäº†ä¼˜åŒ–ä»€ä¹ˆç›®æ ‡ï¼ˆä¾‹å¦‚å®‰å…¨ç¬¬ä¸€ã€æ—¶é—´æœ€çŸ­ã€æ²¹è€—æœ€ä½ç­‰ï¼‰ã€‚



------



**âœ… æ­£å‘ vs é€†å‘**

| **ç±»å‹**              | **ç›®æ ‡**                 | **å·²çŸ¥**                 | **ç»“æœ**         |
| --------------------- | ------------------------ | ------------------------ | ---------------- |
| **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**    | å­¦ä¹ å¦‚ä½•è¡ŒåŠ¨ä»¥æœ€å¤§åŒ–å¥–åŠ± | å¥–åŠ±å‡½æ•°å·²çŸ¥             | ç­–ç•¥ï¼ˆå¦‚ä½•è¡ŒåŠ¨ï¼‰ |
| **é€†å¼ºåŒ–å­¦ä¹ ï¼ˆIRLï¼‰** | å­¦ä¹ å¥–åŠ±å‡½æ•°æœ¬èº«         | è¡Œä¸ºè½¨è¿¹å·²çŸ¥ï¼ˆä¸“å®¶æ¼”ç¤ºï¼‰ | å¥–åŠ±å‡½æ•°         |





------



**ğŸ“¦ é€†å¼ºåŒ–å­¦ä¹ çš„ä¸»è¦ç”¨é€”**

â€‹	â€¢	ğŸ§‘â€ğŸ« **æ¨¡ä»¿å­¦ä¹ ï¼ˆImitation Learningï¼‰**ï¼šå¸Œæœ›æœºå™¨äººæ¨¡ä»¿äººç±»ä¸“å®¶çš„æ“ä½œè¡Œä¸ºï¼Œä½†ä¸åªæ¨¡ä»¿è¡¨é¢åŠ¨ä½œï¼Œè€Œæ˜¯å­¦ä¹ **èƒŒåçš„ç›®çš„**ã€‚

â€‹	â€¢	âš™ï¸ **äººæœºåä½œ**ï¼šç†è§£äººç±»åœ¨åä½œä»»åŠ¡ä¸­è¿½æ±‚çš„ç›®æ ‡ï¼Œä»¥ä¾¿æ›´å¥½åä½œã€‚

â€‹	â€¢	ğŸ¤– **æœºå™¨äººè§„åˆ’**ï¼šè®©æœºå™¨äººç†è§£äººç±»åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„åå¥½ï¼ˆæ¯”å¦‚ç…§é¡¾è€äººæ—¶çš„æ¸©æŸ” vs é«˜æ•ˆï¼‰ã€‚

â€‹	â€¢	ğŸ§  **è®¤çŸ¥ç§‘å­¦**ï¼šç”¨äºæ¨æ–­åŠ¨ç‰©æˆ–äººç±»çš„è¡Œä¸ºåŠ¨æœºã€‚



------



**ğŸ§® æ•°å­¦ä¸Šæ€ä¹ˆåšï¼Ÿ**



ç»™å®šä¸“å®¶çš„è½¨è¿¹ï¼ˆæ¯”å¦‚ä¸€ç³»åˆ—çŠ¶æ€-åŠ¨ä½œå¯¹ï¼‰ï¼ŒIRLå°è¯•æ‰¾åˆ°ä¸€ä¸ªå¥–åŠ±å‡½æ•°ï¼Œä½¿å¾—è¿™äº›è½¨è¿¹æ˜¯â€œæœ€ä¼˜â€çš„ã€‚è¿™é€šå¸¸æ¶‰åŠï¼š

â€‹	1.	å»ºç«‹é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰çš„ç»“æ„ï¼š<S, A, T, Î³>ï¼Œä½†**æ²¡æœ‰å¥–åŠ±å‡½æ•° R**ã€‚

â€‹	2.	å‡è®¾ä¸“å®¶æ‰§è¡Œçš„æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚

â€‹	3.	åˆ©ç”¨è§‚æµ‹åˆ°çš„è¡Œä¸ºï¼Œä¼˜åŒ–å‡ºä¸€ä¸ªä½¿ä¸“å®¶è½¨è¿¹æœ€ä¼˜çš„ Rã€‚



------



**ğŸ“˜ å¸¸è§æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ**

â€‹	â€¢	**MaxEnt IRL**ï¼ˆMaximum Entropy IRLï¼‰ â€“ ç»å…¸æ–¹æ³•ï¼Œå¼•å…¥æœ€å¤§ç†µåŸåˆ™ï¼Œé¼“åŠ±ä¿ç•™è¡Œä¸ºçš„ä¸ç¡®å®šæ€§ã€‚

â€‹	â€¢	**GAIL**ï¼ˆGenerative Adversarial Imitation Learningï¼‰â€“ ä½¿ç”¨GANæ¡†æ¶å¯¹æŠ—å­¦ä¹ ï¼Œæ¨¡ä»¿è¡Œä¸ºåˆ†å¸ƒã€‚

â€‹	â€¢	**AIRL**ï¼ˆAdversarial IRLï¼‰â€“ ä»GAILæ¼”å˜è€Œæ¥ï¼Œå¯ä»¥æ¢å¤å‡ºçœŸå®çš„å¥–åŠ±å‡½æ•°ã€‚



------



æˆ‘ä»¬ç°åœ¨æ¥çœ‹ç¤ºä¾‹ï¼š



------



**ğŸŒ åœºæ™¯è®¾å®šï¼šçœŸå®æœºå™¨äººè½¨è¿¹**



æˆ‘ä»¬å‡è®¾æœºå™¨äººåœ¨ä¸€ä¸ª 2D ç©ºé—´ä¸­æ‰§è¡Œä»»åŠ¡ï¼Œæ¯”å¦‚æŠ“å–ã€æ¨ç‰©ã€é¿éšœç­‰ã€‚æœºå™¨äººè½¨è¿¹ä»¥ state = [x, y] æˆ– [x, y, Î¸] è¡¨ç¤ºã€‚æˆ‘ä»¬é‡‡é›†äº†å¤šæ®µä¸“å®¶æ¼”ç¤ºçš„çŠ¶æ€åºåˆ—ï¼ˆæ²¡æœ‰åŠ¨ä½œ labelï¼‰ï¼Œå¸Œæœ›é€šè¿‡ **ç¥ç»ç½‘ç»œ**æ¥å­¦ä¹ å‡ºéšå«çš„å¥–åŠ±å‡½æ•°ã€‚



------



**ğŸ§  æ•´ä½“æµç¨‹ï¼ˆNeural IRLï¼‰**



æˆ‘ä»¬ä½¿ç”¨ **Maximum Entropy IRL + ç¥ç»ç½‘ç»œ** è¡¨è¾¾å¥–åŠ±å‡½æ•° R(s)ï¼Œè®­ç»ƒæµç¨‹å¦‚ä¸‹ï¼š

â€‹	1.	ä½¿ç”¨ç¥ç»ç½‘ç»œæ‹Ÿåˆ R_\theta(s)

â€‹	2.	ç”¨ soft value iteration è®¡ç®—ç­–ç•¥ Ï€(a|s)

â€‹	3.	ç”¨ Ï€(a|s) æ¨å‡ºæœŸæœ›çŠ¶æ€è®¿é—®é¢‘ç‡

â€‹	4.	æœ€å°åŒ–ä¸“å®¶åˆ†å¸ƒå’Œç­–ç•¥åˆ†å¸ƒçš„å·®å¼‚



------



**ğŸ§ª å®è·µï¼šç”¨ PyTorch å®ç° Neural IRLï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰**



**ğŸ¯ æ•°æ®è®¾å®š**

â€‹	â€¢	çŠ¶æ€æ˜¯ 2D çš„ï¼š[x, y]

â€‹	â€¢	åŠ¨ä½œæ˜¯ç¦»æ•£çš„ï¼šä¸Šä¸‹å·¦å³

â€‹	â€¢	ç¯å¢ƒæ˜¯ä¸€ä¸ªè¿ç»­ç©ºé—´ä¸­çš„æœºå™¨äººå¯¼èˆªä»»åŠ¡

â€‹	â€¢	ä¸“å®¶æ¼”ç¤ºæ˜¯å¤šä¸ªçŠ¶æ€è½¨è¿¹ï¼Œæ¯”å¦‚ï¼š

```
expert_trajs = [
    [[0.0, 0.0], [0.2, 0.0], [0.4, 0.0], [0.6, 0.2], [0.8, 0.4]],
    [[0.1, 0.1], [0.3, 0.1], [0.5, 0.2], [0.7, 0.3], [0.9, 0.5]],
    ...
]
```





------



**âœ… ç¥ç»ç½‘ç»œ IRL å®ç°ï¼ˆæ ¸å¿ƒä»£ç ï¼‰**

```
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

# æ¨¡æ‹Ÿæ•°æ®ï¼ˆä¸“å®¶è½¨è¿¹ï¼‰
expert_trajs = []
for _ in range(10):
    traj = []
    for t in range(10):
        x = t * 0.1
        y = 0.5 * x + 0.05 * np.random.randn()
        traj.append([x, y])
    expert_trajs.append(traj)

# çŠ¶æ€ç©ºé—´é‡‡æ ·
def sample_states(n=1000):
    x = np.random.uniform(0, 1, n)
    y = np.random.uniform(0, 1, n)
    return torch.tensor(np.stack([x, y], axis=1), dtype=torch.float32)

# å¥–åŠ±å‡½æ•°ç½‘ç»œ
class RewardNet(nn.Module):
    def __init__(self, input_dim=2, hidden=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden),
            nn.ReLU(),
            nn.Linear(hidden, hidden),
            nn.ReLU(),
            nn.Linear(hidden, 1)
        )

    def forward(self, s):
        return self.net(s).squeeze(-1)

# ç»éªŒçŠ¶æ€åˆ†å¸ƒï¼ˆä¸“å®¶çš„è®¿é—®é¢‘ç‡ï¼‰
def empirical_state_distribution(trajs):
    states = []
    for traj in trajs:
        states += traj
    states = torch.tensor(states, dtype=torch.float32)
    return states

# è®­ç»ƒ IRLï¼šç”¨æœ€å¤§ç†µ IRL çš„æ€æƒ³
reward_net = RewardNet()
optimizer = optim.Adam(reward_net.parameters(), lr=1e-3)

# è®­ç»ƒ
for epoch in range(500):
    # é‡‡æ ·çŠ¶æ€
    states = sample_states(1024)
    rewards = reward_net(states)

    # ä»ä¸“å®¶è½¨è¿¹ä¸­é‡‡æ ·çŠ¶æ€
    expert_states = empirical_state_distribution(expert_trajs)
    expert_rewards = reward_net(expert_states)

    # Loss: MaxEnt IRL çš„å·®å€¼
    loss = -(expert_rewards.mean() - torch.logsumexp(rewards, dim=0))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 50 == 0:
        print(f"[{epoch}] loss: {loss.item():.4f}")

# âœ… å¯è§†åŒ– reward å‡½æ•°
with torch.no_grad():
    grid_x, grid_y = torch.meshgrid(
        torch.linspace(0, 1, 100),
        torch.linspace(0, 1, 100),
        indexing='xy'
    )
    grid_states = torch.stack([grid_x.flatten(), grid_y.flatten()], dim=1)
    grid_rewards = reward_net(grid_states).reshape(100, 100)

plt.imshow(grid_rewards.numpy(), extent=(0, 1, 0, 1), origin='lower', cmap='plasma')
plt.colorbar()
plt.title("Learned Reward Function")
plt.show()
```





------



**ğŸ“Œ ç»“æœè§£é‡Š**

â€‹	â€¢	ç¥ç»ç½‘ç»œå­¦å‡ºçš„ reward heatmap åº”è¯¥åœ¨ä¸“å®¶è·¯å¾„é™„è¿‘æœ‰é«˜å€¼ï¼Œå…¶ä»–åœ°æ–¹ä½å€¼ã€‚

â€‹	â€¢	æ¢å¥è¯è¯´ï¼Œå®ƒå­¦ä¼šäº†â€œå“ªäº›çŠ¶æ€æ˜¯å¥½çŠ¶æ€â€ï¼Œè€Œæ²¡æœ‰ç›´æ¥æ¨¡ä»¿åŠ¨ä½œã€‚

â€‹	â€¢	è¿™ä¸ª reward å¯ä»¥ç”¨äºåç»­å¼ºåŒ–å­¦ä¹ ï¼Œå†è®­ç»ƒä¸€ä¸ª policy å»æœ€å¤§åŒ–å®ƒã€‚



------



**ğŸ“š å¯æ‰©å±•æ–¹å‘**

â€‹	1.	âœ… æŠŠåŠ¨ä½œä¹Ÿä½œä¸ºè¾“å…¥ R(s, a) æ¥å­¦ä¹ å¥–åŠ±

â€‹	2.	âœ… ç”¨ GAILï¼ˆå¯¹æŠ— IRLï¼‰ä»£æ›¿æœ€å¤§ç†µ IRL

â€‹	3.	âœ… ç”¨çœŸå®æœºå™¨äººè½¨è¿¹ï¼ˆæ¥è‡ª ROSã€Franka æˆ– UR æ•°æ®é›†ï¼‰

â€‹	4.	âœ… åŠ å…¥åŠ¨ä½œæ¨¡å‹å’ŒçœŸå®åŠ¨åŠ›å­¦æ¨ç†



------



ä½ æƒ³æˆ‘ä»¬ä¸‹ä¸€æ­¥æ·±å…¥å“ªä¸ªæ–¹å‘ï¼Ÿæ¯”å¦‚ç”¨ GAILï¼Œè¿˜æ˜¯ä½ æœ‰ç°æˆè½¨è¿¹æ•°æ®æˆ‘ä»¬å¯ä»¥å¥—è¿™ä¸ªæµç¨‹è·‘ï¼Ÿ